Order:
(sequences already trimmed with cutadapt)
gunzip *.gz

usearch -fastq_mergepairs *_R1*.fastq -relabel @ -fastq_minmergelen 100 -fastqout merged.fq

usearch -fastq_filter merged.fq -fastq_maxee 1.0 -fastaout filtered.fa

#for large datasets (even if this isn’t successful for fastx_uniques below, will still scan this combined file for -usearch_global, so make it regardless
#cat filtered_a.fa filtered_b.fa … > filtered.fa

#if too large, run this on each set of filtered sequences, and then cat as next line outlines
usearch -fastx_uniques filtered.fa -fastaout uniques.fa -sizeout

#only for largest sets of 
#cat uniques_a.fa uniques _b.fa uniques _c.fa uniques _d.fa uniques_e.fa > sep_uniques.fa

could not get all filtered to work with the uniques step, and I just used this cat uniques file to sort and then run the unoise steps below (this could be problemeatic because uniques across subsets could be redundant…): 

usearch -sortbysize sep_uniques.fa -fastaout seqs_sorted.fa -minsize 4

usearch -unoise3 seqs_sorted.fa -tabbedout out.txt -zotus denoised.fa

usearch -usearch_global filtered.fq -db denoised.fa -strand plus -id 0.97 -otutabout zotu_table.txt

(have had to do by separate sets because the file is too big…)
